---
title: Python Example
description: 'Get your forecasts with less lines of code than ever. Step-by-step guide. To the fucking moon ðŸš€ðŸŒ™'
---

### API AUTHENTICATION

The ``API-KEY`` grants access to the ``Forecaster`` services. To obtain this credential, we can request it at the developers portal (available at ``https://dev-predictor-apim.developer.azure-api.net``).

```python
API_KEY = "<<INTRODUCE YOUR API_KEY HERE>>"

headers = {
    'Ocp-Apim-Subscription-Key': API_KEY
}
```

### DATA LOADING + MINIMAL ETL

Load the files from the local storage.

```python
import pandas as pd

df = pd.read_parquet('Makridakis_Training.parquet')
```

Before feeding the data to the ``engine``, we must ensure it meets the bare minimum requirements:

> It seems some series don't have the ``time`` field properly filled, which would cause the ``forecaster`` to break down. Thus, we must fix this before proceeding.

> Moreover, some series have different termination dates. Since this is not allowed, we must shift the series to a common termination date.

> We'll also ``resample`` the series to monthly frequency, though this is not compulsory and can be handled by the engine (it would suffice to pass an additional ``frequency`` argument in the configuration file we'll make later), but doing it here will allow us to skip the ``preprocessing`` function altogether, which is often convenient.

``` python
def minimal_etl_to_forecaster(df):

    nans = df.groupby('series_id').apply(lambda x: x.isna().sum())
    nans = nans[nans['timestamp'] > 0].index.tolist()
    df = df[~ df['series_id'].isin(nans)]

    current_date = pd.Timestamp(day = 18, month = 9, year = 2023)

    def shift_to_common_termination_date(series):
        series['timestamp'] = series['timestamp'] - series['timestamp'].max() + current_date
        return series

    df = df.groupby('series_id', group_keys = False).apply(shift_to_common_termination_date)
    df = df.groupby('series_id', group_keys = True).resample('MS', on = 'timestamp').sum(numeric_only = True).reset_index()

    return df

df = minimal_etl_to_forecaster(df)
```

### UPLOAD FILES

Once the ``table`` is ready to go, the only missing piece is the configuration ``json``, which we'll load in from local storage (we might as well just write it as a ``python`` ``dictionary``).

>    time_column_name -> "timestamp"<br>
>    label_column_name -> "value"<br>
>    ids_column_name -> "series_id"<br>
>    univariate -> false<br>
>    covariates -> false<br>
>    frequency -> "MS"

Then, both files must be fed to the ``upload`` endpoint (``https://dev-predictor-apim.azure-api.net/forecaster-v2/upload-files``) with a ``post`` request. Notice that we must also include the ``headers`` with the user's ``API-KEY`` for authentication. If succeeded, the request returns a dictionary with the ``id's`` assigned to the uploaded dataset. This ``id`` must be saved for later, when it will be required for the ``preprocessing`` and ``modelling`` & ``forecast`` functions.

```python
import requests
from io import BytesIO

url = "https://dev-predictor-apim.azure-api.net/forecaster-v2/upload-files"

buffer_to_upload = {}

buffer = BytesIO()
df.to_parquet(buffer)
buffer.seek(0)

buffer_to_upload['labels_training_main'] = buffer
buffer_to_upload['legend'] = open('Makridakis_Legend.json', 'rb')

response = requests.post(url, files = buffer_to_upload, headers = headers)

ids_parallel_run = response.json()
```

We can also upload multiple tables simultaneously by including them all in the ``files`` dictionary (in which case they should all be named beggining with ``labels_training``, just like the code above). This is often used to partition a big dataset into smaller chunks and upload them separately, which would immensely cut down runtimes in the ``modelling`` & ``forecast`` step, thanks to the engine's parallelization capabilities. Notice that the "smaller chunks" could also contain completely unrelated time series, in which case the engine's parallelization capabilities can be used to solve different forecasting problems simultaneously.

