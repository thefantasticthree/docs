---
title: Python Example
description: 'Get your forecasts with less lines of code than ever. Step-by-step guide. To the fucking moon ðŸš€ðŸŒ™'
---

### API AUTHENTICATION

The ``API-KEY`` grants access to the ``Forecaster`` services. To obtain this credential, we can request it at the developers portal (available at ``https://dev-predictor-apim.developer.azure-api.net``).

```python
API_KEY = "<<INTRODUCE YOUR API_KEY HERE>>"

headers = {
    'Ocp-Apim-Subscription-Key': API_KEY
}
```

### DATA LOADING + MINIMAL ETL

Load the files from the local storage.

```python
import pandas as pd

df = pd.read_parquet('Makridakis_Training.parquet')
```

Before feeding the data to the ``engine``, we must ensure it meets the bare minimum requirements:

> It seems some series don't have the ``time`` field properly filled, which would cause the ``forecaster`` to break down. Thus, we must fix this before proceeding.

> Moreover, some series have different termination dates. Since this is not allowed, we must shift the series to a common termination date.

> We'll also ``resample`` the series to monthly frequency, though this is not compulsory and can be handled by the engine (it would suffice to pass an additional ``frequency`` argument in the configuration file we'll make later), but doing it here will allow us to skip the ``preprocessing`` function altogether, which is often convenient.

``` python
def minimal_etl_to_forecaster(df):

    nans = df.groupby('series_id').apply(lambda x: x.isna().sum())
    nans = nans[nans['timestamp'] > 0].index.tolist()
    df = df[~ df['series_id'].isin(nans)]

    current_date = pd.Timestamp(day = 18, month = 9, year = 2023)

    def shift_to_common_termination_date(series):
        series['timestamp'] = series['timestamp'] - series['timestamp'].max() + current_date
        return series

    df = df.groupby('series_id', group_keys = False).apply(shift_to_common_termination_date)
    df = df.groupby('series_id', group_keys = True).resample('MS', on = 'timestamp').sum(numeric_only = True).reset_index()

    return df

df = minimal_etl_to_forecaster(df)
```

### UPLOAD FILES

Once the ``table`` is ready to go, the only missing piece is the configuration ``json``, which we'll load in from local storage (we might as well just write it as a ``python`` ``dictionary``).

>    time_column_name -> "timestamp"<br>
>    label_column_name -> "value"<br>
>    ids_column_name -> "series_id"<br>
>    univariate -> false<br>
>    covariates -> false<br>
>    frequency -> "MS"

Then, both files must be fed to the ``upload`` endpoint (``https://dev-predictor-apim.azure-api.net/forecaster-v2/upload-files``) with a ``post`` request. Notice that we must also include the ``headers`` with the user's ``API-KEY`` for authentication. If succeeded, the request returns a dictionary with the ``id's`` assigned to the uploaded dataset. This ``id`` must be saved for later, when it will be required for the ``preprocessing`` and ``modelling`` & ``forecast`` functions.

```python
import requests
from io import BytesIO

url = "https://dev-predictor-apim.azure-api.net/forecaster-v2/upload-files"

buffer_to_upload = {}

buffer = BytesIO()
df.to_parquet(buffer)
buffer.seek(0)

buffer_to_upload['labels_training_main'] = buffer
buffer_to_upload['legend'] = open('Makridakis_Legend.json', 'rb')

response = requests.post(url, files = buffer_to_upload, headers = headers)

ids_parallel_run = response.json()
```

We can also upload multiple tables simultaneously by including them all in the ``files`` dictionary (in which case they should all be named beggining with ``labels_training``, just like the code above). This is often used to partition a big dataset into smaller chunks and upload them separately, which would immensely cut down runtimes in the ``modelling`` & ``forecast`` step, thanks to the engine's parallelization capabilities. Notice that the "smaller chunks" could also contain completely unrelated time series, in which case the engine's parallelization capabilities can be used to solve different forecasting problems simultaneously.

### PREPROCESSING

Though optional, the ``preprocessing`` step is often convenient because: 1) it grants us relevant insight about the ``time`` ``series`` in our dataset and 2) it transforms the ``time`` ``series`` for better modelling and more precise forecasts. Notice that this step is only compulsory if the uploaded dataset did not meet the bare minimum requirements (which is not our case). A poorly formatted dataset would be one for which at least one of the following holds: 1) the date field is of type ``string`` instead of ``datetime``, 2) the sampling frequency is not consistent or 3) there are missing values in the ``label`` field.

In this example we will only use two ``preprocessing`` functions: 1) ``WhiteNoiseTest``, which measures the ``signal-to-noise`` ratio and 2) ``StationariryTest``, which tests whether the time series are ``stationary`` (that is, their behaviour does not change over time). Notice that these functions do not carry out a transformation of the ``time`` ``series``, but only gather statistical information (which could be useful later). We also include the ``id`` returned by the ``upload`` call as a parameter, so that the engine knows which dataset we want to perform the ``preprocessing`` on.

A request is then made to the preprocessing endpoint: ``https://dev-predictor-apim.azure-api.net/forecaster-v2/launch-preprocessing``, including the user's ``API-KEY`` in the ``header``. The function returns an ``id`` which (uniquely) identifies the ``job``. Notice that this ``id`` is different to the one assigned to the dataset when uploaded in the first step (moreover, the user can make multiple ``preprocessing`` calls upon the same dataset, each of which would return a new ``id``). From now on, we will refer to this ``id`` as the ``job`` ``id``.

```python
import requests

url = "https://dev-predictor-apim.azure-api.net/forecaster-v2/launch-preprocessing"

params = {
    'ids_parallel_run': ids_parallel_run['ids'],
    'transformers': "WhiteNoiseTest-StationarityTest"
}

response = requests.post(url, json = params, headers = headers)
```

Now we'll use the ``get-output`` function to retrieve the ``job``'s results: ``https://dev-predictor-apim.azure-api.net/forecaster-v2/get-output``. The only paramater needed for this function is the ``job`` ``id``, which was assigned in the previous step. Now, this endpoint returns either the results requested or a placeholder message indicating that the job (whose results we wish to retrieve) is yet to finish. Notice the latter will always be the case if one runs this cell immediately after the previous one, since there would have not been enough time for the preprocessing job (or any job for that matter) to finish.

```python
import requests

url = "https://dev-predictor-apim.azure-api.net/forecaster-v2/get-output"

params = {
    'ids_parallel_run': response.json()
}

preprocessing_results = requests.post(url, json = params, headers = headers)
preprocessing_results = preprocessing_results.json()

out = preprocessing_results
```
